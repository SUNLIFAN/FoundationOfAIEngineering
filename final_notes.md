# 人工智能工程基础复习

## 复习提纲

1. AI 定义

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220608233902700-608485224.png" style="zoom: 50%;" />

人工智能标准化白皮书(2018版)中定义:
人工智能是利用数字计算机或者数字计算机控制的机器模拟、延伸和扩展人的智能，感知环境、获取知识并使用知识获得最佳结果的理论、方法、技术及应用系统。

2. 人工智能四个流派

![](https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611115811710-669325378.png)

3. 图灵测试

概念: 让询问人和计算机程序进行5分钟的对话，由询问人来猜测聊天对象是程序还是人，如果程序在超过30%的测试欺骗了询问人，则该程序就算通过了图灵测试。

要通过图灵测试至少还应具有如下能力:

- 自然语言处理(Natural Language Processing), 用于语言交
  流
-  知识表示(Knowledge representation), 用于存储
- 自动推理(automated reasoning), 基于知识进行推理
- 机器学习(Machine Learning), 检测和预测
- 计算机视觉(Computer Vision), 以感知物体
- 机器人学(Robotics), 以操作和移动对象

4.  人工智能发展简史，关键事件，关键人物

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611120225535-1665439456.png" style="zoom: 25%;" />

第一阶段: 20世纪50-80年代
▶ 出现基于数学推理计算机，符号主义盛行
▶ 很多事物不能形式化表达，建模存在局限性
▶ 计算任务复杂性加大后，AI发展遇到瓶颈
第二阶段: 20世纪80年代末-90年代末
▶ 专家系统快速发展，数学模型有突破
▶ 在知识获取、推理能力方面不足
▶ 开发成本偏高，再次进入低谷
第三阶段: 20世纪初-至今
▶ 大数据积聚、理论算法创新、计算力提升
▶ 在多个应用领域获得突破

5. Agent概念

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611120624379-984964293.png" style="zoom: 50%;" />

6. PEAS 描述

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611120521880-1964316318.png" style="zoom: 50%;" />

7. 机器学习概念

一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序在处理T时的性能有所提升。

8. 监督学习、无监督学习概念及其比较

监督学习：训练集给定了正确标签，主要有分类（预测值离散）和回归（预测值连续）

无监督学习：训练集不给定正确标签，主要有降维（如 PCA ） 和聚类

9. 线性回归概念、原理

Hypothesis set 是线性模型，即 $h(x) = \theta^T x, \theta, x \in \mathbb{R}^{n+1},x_0 = 1$

cost function : $J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h(x^{(i)}) - y^{(i)})^2$

objective : $argmin_\theta \quad J(\theta)$ (convex function, local minimal is global minimal)

优化方法：最小二乘法($\theta = (X^TX)^{-1}X^Ty$), 梯度下降法

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611123508325-425143842.png" style="zoom: 50%;" />

梯度下降调优：

- 归一化：简单归一化（除最大值），均值归一化，最大最小值归一化
- 学习率的选择：从某个值开始（如0.01），每迭代若干次就指数衰减

10. 逻辑回归概念、原理

hypothesis set : $h_{\theta}(x) = g(\theta^Tx),g(z) = \frac{1}{1 + e^{-z}}$

意义：$p(y=1|x;\theta) = h_{\theta}(x)$

cost function : $J(\theta) = \frac{1}{m}\sum_{i=1}^m -y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))$(交叉熵损失函数)

objective : $argmin_\theta \quad J(\theta)$ (convex function, local minimal is global minimal)

优化方法：梯度下降或者牛顿法

多分类问题：用 one vs rest

11. 过拟合概念，正则化方法

过拟合：在训练集上效果很好（训练误差很小），但是泛化能力差（对新样本的预测能力差）

解决过拟合的办法：

- 手动筛选特征
- 模型选择（validation）
- 正则化：对模型复杂度作惩罚

12. 梯度下降算法原理，改进的梯度下降方法原理

标准的梯度下降 BGD：每个 step 都要看一遍所有的样本

SGD : 每个 step 只看一个样本

小批量梯度下降：每个 step 看一个小 batch（b 个样本）

13. SVM方法原理，与逻辑回归比较

SVM ：寻找一个超平面尝试以最大间隔分割数据集

软间隔 SVM 的等价损失函数：
$$
J(w, b) = \frac{1}{2}||w||^2 + C\sum_{i=1}^m hinge(x^{(i)}, y^{(i)}) 
$$

$$
hinge(x, y) = 0 (if \quad y(w^Tx  + b) - 1 \ge 0)
$$

$$
hinge(x, y) = 1 - y(w^Tx + b)
$$

核函数方法：用一个函数来表示高维空间中的向量内积。

和逻辑回归的主要区别就是 SVM 尝试用最大间隔来分割数据集，这是它的归纳偏好。

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611132808827-212681819.png" style="zoom:50%;" />

14. K-means方法原理

算法流程

![](https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611133711921-1568876109.png)

代价函数

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611133007115-131395579.png" style="zoom:50%;" />

随机初始化，初始时随机选择 K 个样本作为聚类中心点。

有可能会陷入局部最小，可以多次运行，选择训练误差最小的。

超参数 K 的选择：肘部法则

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611133337999-2084037706.png" style="zoom:33%;" />

15. PCA降维原理

PCA 尝试学习一个线性映射，把一个高维空间的向量映射到一个低维空间。

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611134227092-1876320011.png" style="zoom:33%;" />

经过最优化方法求解之后的结论：对协方差矩阵进行特征值分解，保留前 K 大的特征值对应的特征向量，作为投影矩阵。与要降维的向量相乘就得到降维后的向量。（也可以用奇异值分解）

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611134535709-1574827699.png" style="zoom: 50%;" />

选择 K 的依据：均方误差:$\sum_{i=1}^m||x^i - x^i_{appro}||^2$, 方差: $\sum_{i=1}^m ||x^i||^2$

让均方误差和方差之比尽可能小，比如小于 0.01,或者是让前 K 个特征值的占比大于 0.99

16. 人工神经网络概念原理，参数矩阵计算

数据的流动过程：输入乘权重，相加，经过非线性函数

选择隐层神经元个数：grid search，validation

17. CNN概念，原理，卷积方法，池化方法，卷积核参数计算

![](https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611164310641-1193114366.png)

18. 数据增广方法

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611141018802-905802389.png" style="zoom:33%;" />

19. Dropout方法

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611141250316-1073367736.png" style="zoom: 50%;" />

20. Mobilenet，1X1卷积等原理和优点

1*1 卷积：压缩通道数，比池化层更灵活。

Inception网络主要帮助你解决滤波器尺寸、是否需要卷积或池化层等问题。

Mobile net：适合于移动端及嵌入式；将传统卷积变成深度可分离卷积

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611165927342-1548378029.png" style="zoom: 50%;" />

21. RNN基本原理

RNN 基本单元

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611143319493-1580874292.png" style="zoom:33%;" />

LSTM

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611143546669-479569023.png" style="zoom:33%;" />

<img src="https://img2022.cnblogs.com/blog/2302323/202206/2302323-20220611143638264-1810144160.png" style="zoom: 50%;" />

22. 深度学习模型部署到边缘设备上的挑战和方法

挑战

- **面向不同设备的模型压缩和优化**
- **基于异构硬件资源的系统优化**。
- **隐私保护和模型安全**。
- **主动持续学习**

方法：

- 网络剪枝。找出冗余连接并移除。
- 权值量化。通过减少表示每个权重的比特数来压缩网络。
- 低秩近似。从矩阵分解运算的角度对计算做了优化。

## 2022 试题回忆

### 选择题（10题，20分）

单选题，都比较简单，有一部分来自于 ppt 的选择题。

### 名词解释（6题，30分）

一些名词解释，今年是：监督学习，Adam，正则化，dropout，CNN（还有一个是啥忘了）

### 简答题（50分，忘了几题）

1. 梯度下降原理，优点，调优方法
2. 过拟合的原因，解决办法
3. 一个简单的全连接神经忘了，计算参数数量和参数矩阵大小
4. CNN：卷积层计算参数数量和特征图尺寸
5. 小组作业的主题里面（一些深度学习的技术）选择两个谈谈理解（原理和应用），我们组做的是RNN和 R-CNN，所以我写的是这两个
6. 也是关于最后一次作业：中文手写数字识别的实验，说说如何构造数据集，划分数据集，网络性能调优，作业做了应该没什么问题。
7. 不知道有没有漏掉的，大概记得这些
